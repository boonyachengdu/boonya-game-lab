spring:
  application:
    name: boonya-game-bigdata
  # kafka配置（zookeeper+kafka环境）
  kafka:
    bootstrap-servers: 192.168.158.10:9092 # 虚拟机实例 centos-kafka
    consumer:
      group-id: spark-data-sync-group
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      # 性能优化项
      max-poll-records: 500 # 每次拉取的最大记录数
      fetch-max-wait: 500 # 拉取等待时间
      fetch-min-size: 1 # 最小拉取字节数
    listener:
      ack-mode: manual_immediate
      # 消费者并发数
      concurrency: 3 # 消费者并发数

# CDC Canal
canal:
  server: 192.168.1.100:11111
  destination: example
  username: canal
  password: canal
  filter: .*\\..*

# Presto/Trino 连接配置
presto:
  url: jdbc:trino://your-presto-server:8080
  user: your-username
  catalog: hive
  schema: default
  source: spring-boot-app

# 可选：连接池配置（如果需要使用连接池）
#spring:
#  datasource:
#    presto:
#      jdbc-url: jdbc:trino://your-presto-server:8080
#      username: your-username
#      driver-class-name: io.trino.jdbc.TrinoDriver
#      hikari:
#        maximum-pool-size: 10
#        minimum-idle: 2
#        idle-timeout: 30000
#        max-lifetime: 1800000
#        connection-timeout: 30000

# MySQL 连接配置
mysql:
  url: jdbc:mysql://localhost:3306/bigdata
  user: root
  password: 123456

# 数据湖路径配置
data-lake:
  path: s3a://my-data-lake/

# Spark 配置
spark:
  sql:
    catalogImplementation: hive
  hadoop:
    fs:
      s3a:
        awsCredentialsProvider: com.amazonaws.auth.DefaultAWSCredentialsProviderChain